## conda activate MTLenv
#### python main.py --config_exp exp_configs/single_task.yaml

# DATABASE
'dataset_name': 'NYU '  ### 'NYU', 'Taskonomy'

# EXPERIMENT DETAILS
'Experiment_name': '3_segmentsemantic_trial_4'
'task_list': ['segmentsemantic']  
### 'class_object', 'segmentsemantic', class_scene, 'depth_euclidean', 'surface_normal'

### 1. class_object
### 2. class_scene
### 3. segmentsemantic
### 4. depth_euclidean
### 5. surface_normal
### 6. edge_texture

# SETUP
'setup': 'single_task'
'backbone_model': 'resnet50'    ## efficientnetb7, resnet50
'pretrained': True    # always tru unless train from scratch
'finetuning': False  # finetune from epoch 1 , unfreeze all weights 
'finetuning_after': 15  # number of epochs after which the finetuning of the backbone starts
'checkpoint': False
'checkpoint_folder': "../runs/Taskonomy/4_depth_euclidean_trial_4/"
# COMBINE LOSSES
'comb_loss' : 'uncertainity'   # 'uncertainity',  'sum' # of no use in single task learning

#TRAIN WITH MISSING LABELS FOR META LEARNING
'missing_labels': False
'percent_missing_labels': 1 # write 10, 20, 50 %

# SEGMENTATION PARAMS
'mode': 'multiclass'  ## 'multiclass', binary
'threshold' : 0.15

# BATCH SPECS
'train_batch_size' : 100
'val_batch_size' : 100
'test_batch_size' : 100

# HYPERPARAMETERS
'epochs' : 500
'best_total_loss': 0.001
'num_workers': 4
'model': 'baseline'
'earlystop_patience': 50
'task_earlystop_patience': 30   # for individual tasks , notuseful in single task senario
'loss_weights' : 
    'class_scene': 0.5 
    'class_object': 0.5 

# OPTIMIZER PARAMETERS
'optimizer': 'adamw'    ### 'sgd', 'adamw' , 'adam' 
'optimizer_params': 
    'learning_rate': 0.0003
    'betas': [0.9, 0.999]
    'weight_decay' : 0.01

# SCHEDULER PARAMETERS
'scheduler': 'reduce_on_plateau'
'scheduler_params': 
    'lr_decay_epochs': 50
    'lr_decay_factor': 0.1
    'patience': 50 

# FOR EVALUATION 

'eval_every_10th_epoch': False

#PROFILER
'profiler':
    'wait': 2
    'warmup': 2
    'active': 5
    'repeat': 5


#META HYPERPARAMETERS
'meta':
    'learning_rate': 0.0001
    'betas': [0.9, 0.999]
    'weight_decay' : 0.0001
    'optimizer': 'adamw'
